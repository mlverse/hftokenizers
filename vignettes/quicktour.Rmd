---
title: "Quicktour"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quicktour}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

> Port of the Quicktour vignette in ðŸ¤— tokenizers website. Click [here](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch) for the
original version.

```{r setup}
library(hftokenizers)
```

Letâ€™s have a quick look at the ðŸ¤— Tokenizers library features. The library provides an implementation of todayâ€™s most used tokenizers that is both easy to use and blazing fast.

It can be used to instantiate a pretrained tokenizer but we will start our quicktour by building one from scratch and see how we can train it.

## Build a tokenizer from scratch

To illustrate how fast the ðŸ¤— Tokenizers library is, letâ€™s train a new tokenizer on wikitext-103 (516M of text) in just a few seconds. First things first, you will need to download this dataset and unzip it with:

```{r}
url <- "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip"
fpath <- pins::pin(url)
```
### Training the tokenizer

In this tour, we will build and train a Byte-Pair Encoding (BPE) tokenizer. For 
more information about the different type of tokenizers, check out this guide in 
the ðŸ¤— Transformers documentation. Here, training the tokenizer means it will 
learn merge rules by:

- Start with all the characters present in the training corpus as tokens.
- Identify the most common pair of tokens and merge it into one token.
- Repeat until the vocabulary (e.g., the number of tokens) has reached the size we want.

The main API of the library is the class Tokenizer, here is how we instantiate 
one with a BPE model:

```{r}
tok <- tokenizer$new(model = models_bpe$new(unk_token = "[UNK]"))
```

To train our tokenizer on the wikitext files, we will need to instantiate a trainer, 
in this case a `BpeTrainer`:

```{r}
trainer <- trainers_bpe$new(
  special_tokens=c("[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]")
)
```

We can set the training arguments like `vocab_size` or `min_frequency` (here left 
at their default values of 30,000 and 0) but the most important part is to give 
the `special_tokens` we plan to use later on (they are not used at all during 
training) so that they get inserted in the vocabulary.

> The order in which you write the special tokens list matters: here "[UNK]" will 
  get the ID 0, "[CLS]" will get the ID 1 and so forth.
  
We could train our tokenizer right now, but it wouldnâ€™t be optimal. Without a 
pre-tokenizer that will split our inputs into words, we might get tokens that 
overlap several words: for instance we could get an `"it is"` token since those 
two words often appear next to each other. Using a pre-tokenizer will ensure 
no token is bigger than a word returned by the pre-tokenizer. Here we want to 
train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible 
by splitting on whitespace.  

```{r}
tok$pre_tokenizer <- pre_tokenizers_whitespace()
```

Now, we can just call the `train()` method with any list of files we want to use:

```{r, eval = FALSE}
tok$train(fpath, trainer)
```
```{r include=FALSE}
if (!file.exists("tokenizer-wiki.json"))
  tok$train(fpath, trainer)
```


This should only take a few seconds to train our tokenizer on the full wikitext dataset! To save the tokenizer in one file that contains all its configuration and vocabulary, just use the `save()` method:

```{r, eval = FALSE}
tok$save("tokenizer-wiki.json")
```
```{r include=FALSE}
if (!file.exists("tokenizer-wiki.json"))
  tok$save("tokenizer-wiki.json")
```

and you can reload your tokenizer from that file with the `from_file()` class method:

```{r}
tok <- tokenizer$from_file("tokenizer-wiki.json")
```

### Using the tokenizer


Now that we have trained a tokenizer, we can use it on any text we want with the `encode()` method:

```{r}
output <- tok$encode("Hello, y'all! How are you ðŸ˜ ?")
```

This applied the full pipeline of the tokenizer on the text, returning an `Encoding` object. To learn more about this pipeline, and how to apply (or customize) parts of it, check out [this page](TODO).

This `Encoding` object then has all the attributes you need for your deep learning model (or other). The `tokens` attribute contains the segmentation of your text in tokens:

```{r}
output$tokens
```

Similarly, the `ids` attribute will contain the index of each of those tokens in the tokenizerâ€™s vocabulary:

```{r}
output$ids
```

An important feature of the ðŸ¤— Tokenizers library is that it comes with full alignment tracking, meaning you can always get the part of your original sentence that corresponds to a given token. Those are stored in the offsets attribute of our `Encoding` object. For instance, letâ€™s assume we would want to find back what caused the `"[UNK]"` token to appear, which is the token at index 10 in the list, we can just ask for the offset at the index:

```{r}
output$offsets[[10]]
```

and those are the indices that correspond to the emoji in the original sentence:

```{r}
substr("Hello, y'all! How are you ðŸ˜ ?", 26, 27)
```
### Post-processing

We might want our tokenizer to automatically add special tokens, like `"[CLS]"` or `"[SEP]"`. To do this, we use a post-processor. `TemplateProcessing` is the most commonly used, you just have to specify a template for the processing of single sentences and pairs of sentences, along with the special tokens and their IDs.

When we built our tokenizer, we set `"[CLS]"` and `"[SEP]"` in positions 1 and 2 of our list of special tokens, so this should be their IDs. To double-check, we can use the `token_to_id()` method:

```{r}
tok$token_to_id("[SEP]")
```
Here is how we can set the post-processing to give us the traditional BERT inputs:

```{r}
tok$post_processor <- post_processors_template_processing$new(
  single="[CLS] $A [SEP]",
  pair="[CLS] $A [SEP] $B:1 [SEP]:1",
  special_tokens=list(
     list("[CLS]", tok$token_to_id("[CLS]")),
     list("[SEP]", tok$token_to_id("[SEP]"))
  )
)
```

Letâ€™s go over this snippet of code in more details. First we specify the template for single sentences: those should have the form `"[CLS] $A [SEP]"` where `$A` represents our sentence.

Then, we specify the template for sentence pairs, which should have the form `"[CLS] $A [SEP] $B [SEP]"` where `$A` represents the first sentence and `$B` the second one. The `:1` added in the template represent the type IDs we want for each part of our input: it defaults to 0 for everything (which is why we donâ€™t have `$A:0`) and here we set it to 1 for the tokens of the second sentence and the last `"[SEP]"` token.

Lastly, we specify the special tokens we used and their IDs in our tokenizerâ€™s vocabulary.

To check out this worked properly, letâ€™s try to encode the same sentence as before:

```{r}
tok$encode("Hello, y'all! How are you ðŸ˜ ?")$tokens
```


